{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c48de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture('sample_videos/14-11-03.mp4')\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, 14500)\n",
    "# while True:\n",
    "    \n",
    "#     _, frame = cap.read()\n",
    "#     if _:\n",
    "#         cv2.imshow('video', frame)\n",
    "\n",
    "#         key = cv2.waitKey(17)\n",
    "#         if key == ord('q'):\n",
    "#             break\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebddb089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/riyaz/Desktop/add_detection'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93359edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15050.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap.get(cv2.CAP_PROP_FRAME_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb7f62bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.07831766979937"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap.get(cv2.CAP_PROP_FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa40a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image, ImageStat\n",
    "import math\n",
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture('sample_videos/14-11-03.mp4')\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, 500) - to read from specific frame\n",
    "\n",
    "def rescaleFrame(frame, scale):\n",
    "    \n",
    "    width = int(frame.shape[1] * scale)\n",
    "    height = int(frame.shape[0] * scale)\n",
    "    \n",
    "    dimensions  = (width, height)\n",
    "    \n",
    "    return cv2.resize(frame, dimensions, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "def changeRes(width, height):\n",
    "    # only works for live video\n",
    "    capture.set(3,width)\n",
    "    capture.set(4, height)\n",
    "    # 10 for brightness\n",
    "    \n",
    "def brightness(im_file):\n",
    "    im = Image.open(im_file)\n",
    "    stat = ImageStat.Stat(im)\n",
    "    r,g,b = stat.mean\n",
    "    return math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))    \n",
    "\n",
    "i = 0\n",
    "counter = 0\n",
    "\n",
    "white = cv2.imread('img/img_19.jpg')\n",
    "gray = cv2.cvtColor(white, cv2.COLOR_BGR2GRAY)\n",
    "mask_c = cv2.bitwise_not(gray)\n",
    "\n",
    "folder = \"img/add/\"\n",
    "\n",
    "while True:\n",
    "    \n",
    "    _, frame = cap.read()\n",
    "    img = frame.copy()\n",
    "    \n",
    "#     cv2.rectangle(img, (1550, 55),(1805, 115), (0, 255, 0), 2)\n",
    "    #(from_left_start, from_top_start)(from_left_end, from_top_end)\n",
    "    \n",
    "    img_resized = rescaleFrame(img, 0.5)\n",
    "    \n",
    "    imgWhite = np.ones((img.shape[0], img.shape[1], 3), np.uint8) * 255\n",
    "#     imgWhite = rescaleFrame(imgWhite, 0.5)\n",
    "    \n",
    "    \n",
    "    img_crop = img[55:115, 1550:1805]\n",
    "    \n",
    "    imgWhite[55:115, 1550:1805] = img_crop\n",
    "      \n",
    "    imgWhite = cv2.cvtColor(imgWhite, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    _, imgWhite = cv2.threshold(imgWhite, 150, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    masked_img  = cv2.bitwise_and(img, img, mask=mask_c)\n",
    "    imgb_crop = masked_img[55:115, 1550:1805]\n",
    "    \n",
    "    cv2.imshow('Masked image', masked_img)\n",
    "    cv2.imshow('imageWhhite', imgWhite)\n",
    "#     cv2.imwrite('test.jpg', imgb_crop)\n",
    "#     crp_img = cv2.imread('test.jpg')\n",
    "\n",
    "#     bright = brightness('test.jpg')\n",
    "#     cv2.putText(img, str(np.round(bright,3)), (55, 105), cv2.FONT_HERSHEY_COMPLEX, 1.7, (0, 255, 0), 3)\n",
    "            \n",
    "#     bitwise_not  = cv2.bitwise_not(imgWhite)\n",
    "    \n",
    "#     cv2.imwrite(f'img_{i}.jpg', imgWhite)\n",
    "#     i += 1\n",
    "    \n",
    "    cv2.imshow('video', img)\n",
    "#     cv2.imshow('croped', imgWhite)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord(\"s\"):\n",
    "        counter += 1\n",
    "        cv2.imwrite(f'{folder}/Image_{time.time()}.jpg',imgb_crop)\n",
    "        print(counter)\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "186ab452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8676913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 13:48:27.275405: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-13 13:48:27.425569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/riyaz/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-10-13 13:48:27.425601: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-13 13:48:28.586020: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/riyaz/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-10-13 13:48:28.586089: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/riyaz/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-10-13 13:48:28.586097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f64d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = cv2.imread('img/img_36.jpg')\n",
    "first = cv2.cvtColor(first, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "second = cv2.imread('img/img_71.jpg')\n",
    "second = cv2.cvtColor(second, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b802d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('second', imgblack)\n",
    "if cv2.waitKey(5000):\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec0c34cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('first', first)\n",
    "if cv2.waitKey(5000):\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d981eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):\n",
    "\n",
    "    valid_samples = random.sample(os.listdir(f'img/train/{i}'), 300)\n",
    "    for j in valid_samples:\n",
    "        shutil.move(f'img/train/{i}/{j}', f'img/valid/{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4132c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):\n",
    "\n",
    "    test_samples = random.sample(os.listdir(f'img/train/{i}'), 100)\n",
    "    for j in test_samples:\n",
    "        shutil.move(f'img/train/{i}/{j}', f'img/test/{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f3ff835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'img/train'\n",
    "valid_path = 'img/valid'\n",
    "test_path = 'img/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cb8aa32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5400 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "                                  directory=train_path, target_size=(224,224), batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "54751486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "                                  directory=valid_path, target_size=(224,224), batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bf0a99e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "                                  directory=test_path, target_size=(224,224), batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "97d2dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile = tf.keras.applications.mobilenet.MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "25135bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mobile.layers[-5].output\n",
    "y = tf.reshape(x, shape=(-1, 1024))\n",
    "output = Dense(units=2, activation='softmax')(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5b0e07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=mobile.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0b9f3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:-23]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a95bcc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 112, 112, 32)      864       \n",
      "                                                                 \n",
      " conv1_bn (BatchNormalizatio  (None, 112, 112, 32)     128       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " conv1_relu (ReLU)           (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)     288       \n",
      "                                                                 \n",
      " conv_dw_1_bn (BatchNormaliz  (None, 112, 112, 32)     128       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_1_relu (ReLU)       (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv_pw_1 (Conv2D)          (None, 112, 112, 64)      2048      \n",
      "                                                                 \n",
      " conv_pw_1_bn (BatchNormaliz  (None, 112, 112, 64)     256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_1_relu (ReLU)       (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv_pad_2 (ZeroPadding2D)  (None, 113, 113, 64)      0         \n",
      "                                                                 \n",
      " conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)       576       \n",
      "                                                                 \n",
      " conv_dw_2_bn (BatchNormaliz  (None, 56, 56, 64)       256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_2_relu (ReLU)       (None, 56, 56, 64)        0         \n",
      "                                                                 \n",
      " conv_pw_2 (Conv2D)          (None, 56, 56, 128)       8192      \n",
      "                                                                 \n",
      " conv_pw_2_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_2_relu (ReLU)       (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)      1152      \n",
      "                                                                 \n",
      " conv_dw_3_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv_pw_3 (Conv2D)          (None, 56, 56, 128)       16384     \n",
      "                                                                 \n",
      " conv_pw_3_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv_pad_4 (ZeroPadding2D)  (None, 57, 57, 128)       0         \n",
      "                                                                 \n",
      " conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)      1152      \n",
      "                                                                 \n",
      " conv_dw_4_bn (BatchNormaliz  (None, 28, 28, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_4_relu (ReLU)       (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " conv_pw_4 (Conv2D)          (None, 28, 28, 256)       32768     \n",
      "                                                                 \n",
      " conv_pw_4_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_4_relu (ReLU)       (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)      2304      \n",
      "                                                                 \n",
      " conv_dw_5_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv_pw_5 (Conv2D)          (None, 28, 28, 256)       65536     \n",
      "                                                                 \n",
      " conv_pw_5_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv_pad_6 (ZeroPadding2D)  (None, 29, 29, 256)       0         \n",
      "                                                                 \n",
      " conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)      2304      \n",
      "                                                                 \n",
      " conv_dw_6_bn (BatchNormaliz  (None, 14, 14, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_6_relu (ReLU)       (None, 14, 14, 256)       0         \n",
      "                                                                 \n",
      " conv_pw_6 (Conv2D)          (None, 14, 14, 512)       131072    \n",
      "                                                                 \n",
      " conv_pw_6_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_6_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
      "                                                                 \n",
      " conv_dw_7_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_7 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_7_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
      "                                                                 \n",
      " conv_dw_8_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_8 (Conv2D)          (None, 14, 14, 512)       262144    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " conv_pw_8_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
      "                                                                 \n",
      " conv_dw_9_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_9 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_9_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_10 (DepthwiseConv2D  (None, 14, 14, 512)      4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_10_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_10 (Conv2D)         (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_10_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_11 (DepthwiseConv2D  (None, 14, 14, 512)      4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_11_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_11 (Conv2D)         (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_11_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)      0         \n",
      "                                                                 \n",
      " conv_dw_12 (DepthwiseConv2D  (None, 7, 7, 512)        4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_12_bn (BatchNormali  (None, 7, 7, 512)        2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_12_relu (ReLU)      (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv_pw_12 (Conv2D)         (None, 7, 7, 1024)        524288    \n",
      "                                                                 \n",
      " conv_pw_12_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_12_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " conv_dw_13 (DepthwiseConv2D  (None, 7, 7, 1024)       9216      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_13_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " conv_pw_13 (Conv2D)         (None, 7, 7, 1024)        1048576   \n",
      "                                                                 \n",
      " conv_pw_13_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " global_average_pooling2d_3   (None, 1, 1, 1024)       0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " tf.reshape_3 (TFOpLambda)   (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,230,914\n",
      "Trainable params: 1,864,706\n",
      "Non-trainable params: 1,366,208\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "80f4317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "943f73d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "108/108 - 140s - loss: 0.0189 - accuracy: 0.9911 - val_loss: 5.9015 - val_accuracy: 0.5000 - 140s/epoch - 1s/step\n",
      "Epoch 2/10\n",
      "108/108 - 147s - loss: 9.4060e-04 - accuracy: 0.9996 - val_loss: 0.0050 - val_accuracy: 0.9983 - 147s/epoch - 1s/step\n",
      "Epoch 3/10\n",
      "108/108 - 144s - loss: 1.5011e-05 - accuracy: 1.0000 - val_loss: 8.9390e-05 - val_accuracy: 1.0000 - 144s/epoch - 1s/step\n",
      "Epoch 4/10\n",
      "108/108 - 146s - loss: 6.7778e-04 - accuracy: 0.9996 - val_loss: 0.7910 - val_accuracy: 0.8183 - 146s/epoch - 1s/step\n",
      "Epoch 5/10\n",
      "108/108 - 143s - loss: 1.2838e-04 - accuracy: 1.0000 - val_loss: 1.8175e-06 - val_accuracy: 1.0000 - 143s/epoch - 1s/step\n",
      "Epoch 6/10\n",
      "108/108 - 144s - loss: 2.9207e-05 - accuracy: 1.0000 - val_loss: 1.2815e-07 - val_accuracy: 1.0000 - 144s/epoch - 1s/step\n",
      "Epoch 7/10\n",
      "108/108 - 144s - loss: 8.4032e-06 - accuracy: 1.0000 - val_loss: 5.5630e-08 - val_accuracy: 1.0000 - 144s/epoch - 1s/step\n",
      "Epoch 8/10\n",
      "108/108 - 142s - loss: 5.7979e-06 - accuracy: 1.0000 - val_loss: 2.0067e-08 - val_accuracy: 1.0000 - 142s/epoch - 1s/step\n",
      "Epoch 9/10\n",
      "108/108 - 143s - loss: 2.2638e-06 - accuracy: 1.0000 - val_loss: 1.3312e-08 - val_accuracy: 1.0000 - 143s/epoch - 1s/step\n",
      "Epoch 10/10\n",
      "108/108 - 144s - loss: 1.0978e-06 - accuracy: 1.0000 - val_loss: 1.0927e-08 - val_accuracy: 1.0000 - 144s/epoch - 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d486e92d0>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_batches, validation_data=valid_batches, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "52d1e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x=test_batches, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "904cbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f3e9e720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1], dtype=int32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4fdf4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=test_labels, y_pred = pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0c731733",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(cm, display_labels= ['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ef5e18ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f6d4853add0>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "disp.plot()\n",
    "\n",
    "# disp.plot(cmap = plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1c95468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tv_add_224.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fba9f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_test = cv2.imread('img/data/notadd/Image_1665636243.678401.jpg')\n",
    "im = cv2.cvtColor(im_test, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0fa53c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 255, 3)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b3336ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,2):\n",
    "\n",
    "#     images = os.listdir(f'img/test/{i}')\n",
    "#     for j in images:\n",
    "\n",
    "#         im_test = cv2.imread(f'img/test/{i}/{j}')\n",
    "#         im = cv2.cvtColor(im_test, cv2.COLOR_BGR2RGB)\n",
    "#         imgWhite = np.ones((224, 224, 3), np.uint8) * 255\n",
    "\n",
    "#         ar = 60/255\n",
    "#         k = 224 / 255\n",
    "#         hCal = math.ceil(k * 60)\n",
    "#         imgResize = cv2.resize(im, (224, hCal))\n",
    "#         imgResizeShape = imgResize.shape\n",
    "#         hGap = math.ceil((224 - hCal) / 2)\n",
    "#         imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "        \n",
    "#         cv2.imwrite(f'img/test/{i}/{j}',imgWhite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a291aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4682049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgWhite = np.ones((224, 224, 3), np.uint8) * 255\n",
    "\n",
    "ar = 60/255\n",
    "k = 224 / 255\n",
    "hCal = math.ceil(k * 60)\n",
    "imgResize = cv2.resize(im, (224, hCal))\n",
    "imgResizeShape = imgResize.shape\n",
    "hGap = math.ceil((224 - hCal) / 2)\n",
    "imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "cv2.imshow('imagewhite', imgWhite)\n",
    "if cv2.waitKey(5000):\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "961c1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm=imgWhite/255.0\n",
    "reshap=np.reshape(norm,(1,224,224,3))\n",
    "reshap = np.vstack([reshap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "42dd8eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(reshap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bf7367e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.2188152e-11, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "19890091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e28c858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 323ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "{}\n",
      "Total_time:  0:00:14.442339\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image, ImageStat\n",
    "import math\n",
    "import time\n",
    "import threading\n",
    "import datetime\n",
    "\n",
    "cap = cv2.VideoCapture('sample_videos/15-40-59.mp4')\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, 14500) # to start reading from a specific frame\n",
    "\n",
    "def rescaleFrame(frame, scale):\n",
    "    \n",
    "    width = int(frame.shape[1] * scale)\n",
    "    height = int(frame.shape[0] * scale)\n",
    "    \n",
    "    dimensions  = (width, height)\n",
    "    \n",
    "    return cv2.resize(frame, dimensions, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "def predict(*args):\n",
    "    \n",
    "    global tym, timer\n",
    "    tym = True\n",
    "    time.sleep(0.05)\n",
    "    tym = False\n",
    "    timer = threading.Timer(1, predict)\n",
    "    timer.start()    \n",
    "\n",
    "predict()\n",
    "\n",
    "white = cv2.imread('img/img_19.jpg')\n",
    "gray = cv2.cvtColor(white, cv2.COLOR_BGR2GRAY)\n",
    "mask_c = cv2.bitwise_not(gray)\n",
    "\n",
    "model = tf.keras.models.load_model('tv_add_224.h5')\n",
    "labels = ['Not Ad', 'Ad','']\n",
    "\n",
    "set_val = set([])\n",
    "val = 2\n",
    "start = None\n",
    "end = None\n",
    "\n",
    "video_start = datetime.datetime.now()\n",
    "temp = video_start-video_start\n",
    "add_start = temp\n",
    "time_elapsed = temp\n",
    "start_time = video_start\n",
    "\n",
    "\n",
    "add_dict = {}\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "   \n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    if _:\n",
    "        img = frame.copy()\n",
    "\n",
    "        masked_img  = cv2.bitwise_and(img, img, mask=mask_c)\n",
    "        imgb_crop = masked_img[55:115, 1550:1805]\n",
    "\n",
    "        if tym:\n",
    "            imgWhite_crop = np.ones((224, 224, 3), np.uint8) * 255\n",
    "\n",
    "            ar = 60/255\n",
    "            k = 224 / 255\n",
    "            hCal = math.ceil(k * 60)\n",
    "            imgResize = cv2.resize(imgb_crop, (224, hCal))\n",
    "            imgResizeShape = imgResize.shape\n",
    "            hGap = math.ceil((224 - hCal) / 2)\n",
    "            imgWhite_crop[hGap:hCal + hGap, :] = imgResize\n",
    "\n",
    "\n",
    "            norm=imgWhite_crop/255.0\n",
    "            reshap=np.reshape(norm,(1,224,224,3))\n",
    "            reshap = np.vstack([reshap])\n",
    "\n",
    "            pred = model.predict(reshap)\n",
    "            val = np.argmax(pred)\n",
    "\n",
    "            set_val.add(val)\n",
    "\n",
    "            if len(set_val) == 2 and val == 1:\n",
    "                print('add starting')\n",
    "                start = 'start'\n",
    "                start_time = datetime.datetime.now()\n",
    "                add_start = start_time - video_start\n",
    "\n",
    "                set_val = set([])\n",
    "\n",
    "            if len(set_val) == 2 and val == 0:\n",
    "                print('add completed')\n",
    "                end = 'finished'\n",
    "                end_time = datetime.datetime.now()\n",
    "                add_end = end_time - video_start\n",
    "                if start:\n",
    "                    time_elapsed = end_time-start_time\n",
    "\n",
    "                    start = None\n",
    "                else:\n",
    "                    time_elapsed = end_time-video_start\n",
    "\n",
    "                add_dict[f'ad_{i+1}'] = (f'start: {add_start}, stop: {add_end}, duration: {time_elapsed}')\n",
    "                i += 1\n",
    "                set_val = set([])\n",
    "\n",
    "\n",
    "        cv2.putText(img, labels[val], (55, 125), cv2.FONT_HERSHEY_COMPLEX, 1.7, (0, 255, 0), 3)\n",
    "        cv2.putText(img, (str(datetime.datetime.now()-video_start)[:8]),(55, 70),cv2.FONT_HERSHEY_SIMPLEX, 1.7,(0, 255, 0), 3)\n",
    "\n",
    "        img_resized = rescaleFrame(img, 0.5)\n",
    "        cv2.imshow('video', img_resized)\n",
    "#         cv2.imshow('inverted', imgWhite_crop)\n",
    "\n",
    "        key = cv2.waitKey(19)\n",
    "\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        if start or (len(set_val) == 1 and val == 1):\n",
    "            print('add completed')\n",
    "            end = 'finished'\n",
    "            end_time = datetime.datetime.now()\n",
    "            add_end = end_time - video_start\n",
    "            time_elapsed = end_time-start_time\n",
    "            add_dict[f'ad_{i+1}'] = (f'start: {add_start}, stop: {add_end}, duration: {time_elapsed}')\n",
    "        break    \n",
    "\n",
    "video_end = datetime.datetime.now()\n",
    "total_time = video_end - video_start   \n",
    "\n",
    "print(add_dict)\n",
    "print('Total_time: ', total_time)\n",
    "\n",
    "timer.cancel()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5928c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
